{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import required libraries and packages\nimport os, shutil\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom zipfile import ZipFile\nimport gc\nimport cv2\nimport cupy as cp\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_modality_lut\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:55:12.218091Z","iopub.execute_input":"2023-10-25T18:55:12.218470Z","iopub.status.idle":"2023-10-25T18:55:12.224742Z","shell.execute_reply.started":"2023-10-25T18:55:12.218443Z","shell.execute_reply":"2023-10-25T18:55:12.223776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    SEED = 42\n    IMAGE_SIZE = [256, 256]\n    RESIZE_DIM = 256\n    TARGET_COLS = [\n        'bowel_injury', 'extravasation_injury',\n        'kidney_healthy', 'kidney_low', 'kidney_high',\n        'liver_healthy', 'liver_low', 'liver_high',\n        'spleen_healthy', 'spleen_low', 'spleen_high',\n    ]\n#Create an instance of the Config Class\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:17.703894Z","iopub.execute_input":"2023-10-25T18:48:17.704437Z","iopub.status.idle":"2023-10-25T18:48:17.709635Z","shell.execute_reply.started":"2023-10-25T18:48:17.704409Z","shell.execute_reply":"2023-10-25T18:48:17.708675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Dataset Overview:\n\nThe dataset supplied for this competition is initially in DICOM format, a standard for medical imaging. However, for the purpose of our training pipeline, we have opted to use PNG images extracted from the DICOM format.\n\n**Reasoning:**\n\n1. **Compatibility and Standardization:**\n   - **DICOM (Digital Imaging and Communications in Medicine):** DICOM is a widely accepted standard for medical imaging, ensuring compatibility across different medical devices and systems. However, it is not inherently suitable for deep learning frameworks.\n   - **PNG (Portable Network Graphics):** PNG, on the other hand, is a format specifically designed for the web but is easily interpretable by various image processing libraries used in machine learning frameworks. Using PNG allows us to leverage the flexibility and ease of integration offered by common image handling tools.\n\n2. **Simplified Preprocessing:**\n   - **DICOM:** DICOM files often contain a plethora of metadata and additional information that might not be relevant for our specific machine learning task. Extracting and processing this information can be computationally intensive.\n   - **PNG:** By converting DICOM to PNG, we streamline the preprocessing pipeline. PNG images typically retain only the essential visual information, making it easier to handle and reducing the complexity of our data preparation steps.\n\n3. **Community Support:**\n   - **PNG:** The machine learning community has developed extensive tooling and resources around common image formats like PNG. This includes pre-trained models, data augmentation techniques, and a wealth of knowledge on best practices. Utilizing PNG aligns our approach with community standards, facilitating collaboration and knowledge exchange.\n\n4. **Visualization and Interpretability:**\n   - **PNG:** PNG images are easily viewable with standard image viewers and can be more intuitively interpreted by researchers and medical professionals. This facilitates the collaborative analysis of the dataset and the validation of model outputs.\n\n5. **Storage Efficiency:**\n   - **PNG:** While DICOM is efficient for storage of medical imaging data, for our specific machine learning application, PNG can be more storage-friendly. PNG compression is lossless, maintaining image quality while potentially reducing storage requirements compared to DICOM.\n\nIn summary, the decision to work with PNG images derived from DICOM is driven by a combination of technical compatibility, ease of preprocessing, community support, visualization benefits, and storage considerations tailored to the requirements of our machine learning workflow.","metadata":{"execution":{"iopub.status.busy":"2023-10-17T21:09:46.082600Z","iopub.execute_input":"2023-10-17T21:09:46.082887Z","iopub.status.idle":"2023-10-17T21:09:46.127678Z","shell.execute_reply.started":"2023-10-17T21:09:46.082862Z","shell.execute_reply":"2023-10-17T21:09:46.126483Z"}}},{"cell_type":"code","source":"MAIN_FOLDER = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\nIMAGE_DIR = '/tmp/dataset/rsna-atd'\nTRAIN_PATH = \"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/\"\nStride = 10\n!ls {MAIN_FOLDER}","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:17.711204Z","iopub.execute_input":"2023-10-25T18:48:17.711556Z","iopub.status.idle":"2023-10-25T18:48:18.656551Z","shell.execute_reply.started":"2023-10-25T18:48:17.711525Z","shell.execute_reply":"2023-10-25T18:48:18.655479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metadata Overview\n\n**The train.csv file provides comprehensive meta-information, including:**\n\n* **patient_id:** A distinctive identification code assigned to each patient.\n* **series_id:** A unique identifier for each scan, facilitating traceability.\n* **instance_number:** Denotes the image sequence within a scan. Notably, the starting instance number is often above zero due to the initial cropping of scans to the abdomen.\n* **[bowel/extravasation]_[healthy/injury]:** Presents binary targets for two distinct injury types, namely bowel and extravasation.\n* **[kidney/liver/spleen]_[healthy/low/high]:** Offers a nuanced perspective with three target levels for three injury types: kidney, liver, and spleen.\n* **any_injury:** Indicates whether a patient experienced any form of injury during the course of observation.\n\nThis structured metadata is instrumental for comprehending and navigating the dataset. The patient_id and series_id enable unique patient and scan identification, while instance_number provides insights into the sequential order of images within scans. The injury-related fields afford a detailed breakdown, categorizing injuries into binary and trinary classifications. Lastly, any_injury serves as a concise flag denoting the overall injury status of each patient. This clear delineation of metadata categories enhances the dataset's interpretability, facilitating efficient analysis and model training.","metadata":{}},{"cell_type":"code","source":"#Read CSV files\nmeta_train_df = pd.read_csv(f'{MAIN_FOLDER}/train_series_meta.csv')\n\n#Checking if patients are repeated by finding the number of unique patient IDs\nnum_train_rows = meta_train_df.shape[0]\nunique_train_patients = meta_train_df['patient_id'].nunique()\n\nprint(f'{num_train_rows=}')\nprint(f'{unique_train_patients=}')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:18.657916Z","iopub.execute_input":"2023-10-25T18:48:18.658232Z","iopub.status.idle":"2023-10-25T18:48:18.673924Z","shell.execute_reply.started":"2023-10-25T18:48:18.658205Z","shell.execute_reply":"2023-10-25T18:48:18.673096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read CSV files\nmeta_test_df = pd.read_csv(f'{MAIN_FOLDER}/test_series_meta.csv')\n\n#Checking if patients are repeated by finding the number of unique patient IDs\nnum_test_rows = meta_test_df.shape[0]\nunique_test_patients = meta_train_df['patient_id'].nunique()\n\nprint(f'{num_test_rows=}')\nprint(f'{unique_test_patients=}')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:18.676340Z","iopub.execute_input":"2023-10-25T18:48:18.676602Z","iopub.status.idle":"2023-10-25T18:48:18.685810Z","shell.execute_reply.started":"2023-10-25T18:48:18.676578Z","shell.execute_reply":"2023-10-25T18:48:18.685012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read CSV files\ntrain_df = pd.read_csv(os.path.join(MAIN_FOLDER, 'train.csv'))\nimg_lvl_df = pd.read_csv(os.path.join(MAIN_FOLDER, 'image_level_labels.csv'))\n\n#Merge DataFrames\ntrain_df = pd.merge(train_df, img_lvl_df, on='patient_id', how='right')\n\n#Construct image_path using os.path.join\ntrain_df['image_path'] = train_df.apply(\n    lambda row: os.path.join(MAIN_FOLDER, 'train_images', str(row['patient_id']),\n                             str(row['series_id']), f\"{row['instance_number']}.dcm\"),\n    axis=1\n)\n\n#Drop duplicates\ntrain_df.drop_duplicates()\n\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:18.686893Z","iopub.execute_input":"2023-10-25T18:48:18.687189Z","iopub.status.idle":"2023-10-25T18:48:19.018670Z","shell.execute_reply.started":"2023-10-25T18:48:18.687164Z","shell.execute_reply":"2023-10-25T18:48:19.017723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if patients are repeated by finding the number of unique patient IDs for train data\nnum_rows = train_df.shape[0]\nunique_patients = train_df['patient_id'].nunique()\n\nprint(f'{num_rows=}')\nprint(f'{unique_patients=}')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:19.019880Z","iopub.execute_input":"2023-10-25T18:48:19.020209Z","iopub.status.idle":"2023-10-25T18:48:19.026210Z","shell.execute_reply.started":"2023-10-25T18:48:19.020174Z","shell.execute_reply":"2023-10-25T18:48:19.025320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Glob for DICOM files in the specified directory\ntest_paths = glob('/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/*/*/*dcm')\n\n#Create a DataFrame to organize the test dataset\ntest_df = pd.DataFrame(test_paths, columns=[\"image_path\"])\n\n#Extract patient_id, series_id, and instance_number from the file path and convert to integer\ntest_df['patient_id'] = test_df.image_path.map(lambda x: x.split('/')[-3]).astype(int)\ntest_df['series_id'] = test_df.image_path.map(lambda x: x.split('/')[-2]).astype(int)\ntest_df['instance_number'] = test_df.image_path.map(lambda x: x.split('/')[-1].replace('.dcm','')).astype(int)\n\nprint('Test:')\nprint(f'# Size: {len(test_df)}')\ndisplay(test_df.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:19.027529Z","iopub.execute_input":"2023-10-25T18:48:19.027878Z","iopub.status.idle":"2023-10-25T18:48:19.046996Z","shell.execute_reply.started":"2023-10-25T18:48:19.027845Z","shell.execute_reply":"2023-10-25T18:48:19.046134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if patients are repeated by finding the number of unique patient IDs for test data\nnum_rows = test_df.shape[0]\nunique_patients = test_df['patient_id'].nunique()\n\nprint(f'{num_rows=}')\nprint(f'{unique_patients=}')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:19.047936Z","iopub.execute_input":"2023-10-25T18:48:19.048210Z","iopub.status.idle":"2023-10-25T18:48:19.053493Z","shell.execute_reply.started":"2023-10-25T18:48:19.048188Z","shell.execute_reply":"2023-10-25T18:48:19.052590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DICOM to PNG pipeline","metadata":{}},{"cell_type":"code","source":"#Remove the directory and its contents (if it exists)\n!rm -r {IMAGE_DIR}\n\n#Create 'train_images' and 'test_images' directories within IMAGE_DIR\nos.makedirs(f'{IMAGE_DIR}/train_images', exist_ok=True)\nos.makedirs(f'{IMAGE_DIR}/test_images', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:19.054428Z","iopub.execute_input":"2023-10-25T18:48:19.054654Z","iopub.status.idle":"2023-10-25T18:48:20.394397Z","shell.execute_reply.started":"2023-10-25T18:48:19.054635Z","shell.execute_reply":"2023-10-25T18:48:20.393179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n    \"\"\"\n    Standardize pixel array based on DICOM metadata.\n    \n    Args:\n        dcm (pydicom.dataset.FileDataset): DICOM file dataset.\n    \n    Returns:\n        numpy.ndarray: Standardized pixel array.\n    \"\"\"\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n    pixel_array = dcm.pixel_array\n    if dcm.PixelRepresentation == 1:\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >> bit_shift\n\n    intercept = float(dcm.RescaleIntercept)\n    slope = float(dcm.RescaleSlope)\n    center = int(dcm.WindowCenter)\n    width = int(dcm.WindowWidth)\n    low = center - width / 2\n    high = center + width / 2    \n    \n    pixel_array = (pixel_array * slope) + intercept\n    pixel_array = np.clip(pixel_array, low, high)\n\n    return pixel_array\n\ndef read_xray(path, fix_monochrome=True):\n    \"\"\"\n    Read X-ray DICOM file, standardize pixel array, and normalize the values.\n    \n    Args:\n        path (str): File path to the X-ray DICOM file.\n        fix_monochrome (bool): Flag to fix monochrome inversion if PhotometricInterpretation is 'MONOCHROME1'.\n    \n    Returns:\n        numpy.ndarray: Normalized pixel values of the X-ray image.\n    \"\"\"\n    dicom = pydicom.dcmread(path)\n    data = standardize_pixel_array(dicom)\n\n    #Normalize pixel values\n    data = (data - np.min(data)) / (np.max(data) + 1e-5)\n\n    #Fix monochrome if needed\n    if fix_monochrome and dicom.PhotometricInterpretation == 'MONOCHROME1':\n        data = 1.0 - data\n\n    return data\n\ndef resize_and_save(file_path, resize_dim=(config.RESIZE_DIM, config.RESIZE_DIM)):\n    \"\"\"\n    Resize image, convert to uint8, and save to a new location.\n    Returns patient, study, image IDs, original width, and height.\n\n    Args:\n        file_path (str): File path to the X-ray DICOM file.\n        resize_dim (tuple): Dimensions for resizing the image.\n\n    Returns:\n        tuple: Patient ID, Study ID, Image ID, Original Width, Original Height.\n    \"\"\"\n    img = read_xray(file_path)\n\n    # Resize using Cupy on GPU\n    img_gpu = cp.asarray(img)\n    img_resized_gpu = cp.asnumpy(cv2.resize(img_gpu, resize_dim, cv2.INTER_LINEAR))\n    \n    # Scale to uint8 on GPU\n    img_resized_gpu = cp.asnumpy((img_resized_gpu * 255).astype(np.uint8))\n\n    # Extract patient, study, and image IDs\n    sub_path = file_path.split(\"/\", 4)[-1].split('.dcm')[0] + '.png'\n    infos = sub_path.split('/')\n    pid = infos[-3]\n    sid = infos[-2]\n    iid = infos[-1]; iid = iid.replace('.png','')\n    \n    # Create new path\n    new_path = os.path.join(IMAGE_DIR, sub_path)\n    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n    \n    # Save the resized image directly\n    cv2.imwrite(new_path, img_resized_gpu)\n    \n    return pid, sid, iid, img.shape[1], img.shape[0]\n\ndef resize_and_save_with_visualization(file_path, resize_dim=(config.RESIZE_DIM, config.RESIZE_DIM), plot_frequency=1000):\n    \"\"\"\n    Resize image, convert to uint8, and save to a new location with visualization.\n\n    Args:\n        file_path (str): File path to the X-ray DICOM file.\n        resize_dim (tuple): Dimensions for resizing the image.\n        plot_frequency (int): Frequency of plotting images during processing.\n\n    Returns:\n        tuple: Patient ID, Study ID, Image ID, Original Width, Original Height.\n    \"\"\"\n    dicom = pydicom.dcmread(file_path)\n    \n    # Standardize pixel array\n    img = standardize_pixel_array(dicom)\n    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n\n    # Flip image if MONOCHROME1\n    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        img = 1 - img\n\n    h, w = img.shape[:2]\n    \n    # Resize using Cupy and convert to NumPy\n    img_resized = cv2.resize(img, resize_dim, cv2.INTER_LINEAR)\n    \n    # Scale to uint8 \n    img_resized = (img_resized * 255).astype(np.uint8)\n    \n    # Extract patient, study, and image IDs\n    sub_path = file_path.split(\"/\", 4)[-1].split('.dcm')[0] + '.png'\n    infos = sub_path.split('/')\n    pid = infos[-3]\n    sid = infos[-2]\n    iid = infos[-1]; iid = iid.replace('.png','')\n    \n    # Create new path\n    new_path = os.path.join(IMAGE_DIR, sub_path)\n    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n    \n    # Save the resized image directly\n    cv2.imwrite(new_path, img_resized)\n\n    # Plotting\n    if not (len(os.listdir(os.path.dirname(file_path))) % plot_frequency):\n        plt.figure(figsize=(5, 5))\n        plt.imshow(img_resized, cmap=\"gray\")\n        plt.title(f\"Patient {pid} - Study {sid} - Frame {iid}\")\n        plt.axis(False)\n        plt.show()\n\n    return pid, sid, iid, w, h\n\ndef process_images_with_resizing_and_visualization(file_paths):\n    \"\"\"\n    Process images in parallel with resizing, saving, and visualization.\n\n    Args:\n        file_paths (list): List of file paths to DICOM images.\n\n    Returns:\n        list: A list containing the sizes of processed images.\n    \"\"\"\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        # Use ThreadPoolExecutor to parallelize image processing using the resize_and_save function\n        image_sizes = list(tqdm(\n            executor.map(resize_and_save_with_visualization, file_paths),\n            total=len(file_paths),\n            leave=True\n        ))\n    return image_sizes\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:20.396091Z","iopub.execute_input":"2023-10-25T18:48:20.396398Z","iopub.status.idle":"2023-10-25T18:48:20.419229Z","shell.execute_reply.started":"2023-10-25T18:48:20.396372Z","shell.execute_reply":"2023-10-25T18:48:20.418344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_train_paths = train_df.image_path.tolist()\n\n#Process images in parallel and measure time\nstart_time = time.time()\ntrain_img_sizes = process_images_with_resizing_and_visualization(file_train_paths)\nend_time = time.time()\n\nprint(f\"Total time taken: {end_time - start_time} seconds\")\n\n# Perform garbage collection to free up resources\ngc.collect()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:48:20.420558Z","iopub.execute_input":"2023-10-25T18:48:20.420883Z","iopub.status.idle":"2023-10-25T18:51:40.517083Z","shell.execute_reply.started":"2023-10-25T18:48:20.420854Z","shell.execute_reply":"2023-10-25T18:51:40.516212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unpack imgsize_train list of tuples into separate lists\npid, sid, iid, width, height = zip(*train_img_sizes)\n\n#Create a DataFrame with the extracted information\nmeta_df = pd.DataFrame({\n    'patient_id_meta': pid,\n    'series_id_meta': sid,\n    'instance_number_meta': iid,\n    'width': width,\n    'height': height\n})\n\n#Convert specific columns to integer type\ncolumns_to_convert = ['patient_id_meta', 'series_id_meta', 'instance_number_meta']\nmeta_df[columns_to_convert] = meta_df[columns_to_convert].astype(int)\n\n#Merge the DataFrames based on specific columns or index\ntrain_df = pd.concat([train_df, meta_df], axis=1) \n\n#Drop the redundant columns\ntrain_df = train_df.drop(columns=['patient_id_meta', 'series_id_meta', 'instance_number_meta'])\n\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:40.518286Z","iopub.execute_input":"2023-10-25T18:51:40.518570Z","iopub.status.idle":"2023-10-25T18:51:40.566659Z","shell.execute_reply.started":"2023-10-25T18:51:40.518546Z","shell.execute_reply":"2023-10-25T18:51:40.565803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_test_paths = test_df.image_path.tolist()\n\n#Process images in parallel and measure time\nstart_time = time.time()\ntest_img_sizes = process_images_with_resizing_and_visualization(file_test_paths)\nend_time = time.time()\n\nprint(f\"Total time taken: {end_time - start_time} seconds\")\n\n# Perform garbage collection to free up resources\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:40.569948Z","iopub.execute_input":"2023-10-25T18:51:40.570256Z","iopub.status.idle":"2023-10-25T18:51:40.704580Z","shell.execute_reply.started":"2023-10-25T18:51:40.570222Z","shell.execute_reply":"2023-10-25T18:51:40.703652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unpack imgsize_train list of tuples into separate lists\npid, sid, iid, width, height = zip(*test_img_sizes)\n\n#Create a DataFrame with the extracted information\nmeta_df = pd.DataFrame({\n    'patient_id_meta': pid,\n    'series_id_meta': sid,\n    'instance_number_meta': iid,\n    'width': width,\n    'height': height\n})\n\n#Convert specific columns to integer type\ncolumns_to_convert = ['patient_id_meta', 'series_id_meta', 'instance_number_meta']\nmeta_df[columns_to_convert] = meta_df[columns_to_convert].astype(int)\n\n#Merge the DataFrames based on specific columns or index\ntest_df = pd.concat([test_df, meta_df], axis=1) \n\n#Drop the redundant columns\ntest_df = test_df.drop(columns=['patient_id_meta', 'series_id_meta', 'instance_number_meta'])\n\ntest_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:40.705522Z","iopub.execute_input":"2023-10-25T18:51:40.705772Z","iopub.status.idle":"2023-10-25T18:51:40.722139Z","shell.execute_reply.started":"2023-10-25T18:51:40.705750Z","shell.execute_reply":"2023-10-25T18:51:40.721149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filter the DataFrame based on the condition\nfiltered_train_df = train_df[train_df.width < 700]\n#Get the image path from the filtered DataFrame\nimage_path = filtered_train_df.image_path.iloc[0].replace(MAIN_FOLDER, IMAGE_DIR).replace('.dcm', '.png')\n#Read the image using cv2\nimg = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n\n#Plot the image\nplt.figure(figsize=(10, 10))\nplt.imshow(img, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:40.723289Z","iopub.execute_input":"2023-10-25T18:51:40.723621Z","iopub.status.idle":"2023-10-25T18:51:41.056620Z","shell.execute_reply.started":"2023-10-25T18:51:40.723596Z","shell.execute_reply":"2023-10-25T18:51:41.055741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filter the DataFrame based on the condition\nfiltered_train_df = train_df[train_df.width > 700]\n#Check if there are any matching rows\nif not filtered_train_df.empty:\n    #Get the image path from the filtered DataFrame\n    image_path = filtered_train_df.image_path.iloc[0].replace(MAIN_FOLDER, IMAGE_DIR).replace('.dcm', '.png')\n    #Read the image using cv2\n    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n    #Plot the image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(img, cmap='gray')\n    plt.show()\nelse:\n    print(\"No images found for the specified condition.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:41.057716Z","iopub.execute_input":"2023-10-25T18:51:41.057990Z","iopub.status.idle":"2023-10-25T18:51:41.376847Z","shell.execute_reply.started":"2023-10-25T18:51:41.057945Z","shell.execute_reply":"2023-10-25T18:51:41.375959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating CSV","metadata":{}},{"cell_type":"code","source":"#Save DataFrames to CSV files\ntrain_df.to_csv(f'{IMAGE_DIR}/train.csv', index=False)\ntest_df.to_csv(f'{IMAGE_DIR}/test.csv', index=False)\n\n#Copy additional metadata files to the target directory\nshutil.copy(f'{MAIN_FOLDER}/train_series_meta.csv', f'{IMAGE_DIR}/')\nshutil.copy(f'{MAIN_FOLDER}/test_series_meta.csv', f'{IMAGE_DIR}/')\nshutil.copy(f'{MAIN_FOLDER}/sample_submission.csv', f'{IMAGE_DIR}/')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:41.378121Z","iopub.execute_input":"2023-10-25T18:51:41.378452Z","iopub.status.idle":"2023-10-25T18:51:41.518022Z","shell.execute_reply.started":"2023-10-25T18:51:41.378422Z","shell.execute_reply":"2023-10-25T18:51:41.517218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#List directory contents\n!ls -al {IMAGE_DIR}","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:51:41.519041Z","iopub.execute_input":"2023-10-25T18:51:41.519321Z","iopub.status.idle":"2023-10-25T18:51:42.460176Z","shell.execute_reply.started":"2023-10-25T18:51:41.519297Z","shell.execute_reply":"2023-10-25T18:51:42.459141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset","metadata":{}},{"cell_type":"code","source":"#Create a ZipFile object for writing\nzipObj = ZipFile(f'/kaggle/working/rsna-acd-processed.zip', 'w')\n\n#Gather file paths from the specified IMAGE_DIR and its subdirectories\nfile_paths = glob(f'{IMAGE_DIR}/**/*', recursive=True)\n\n#Print the total number of files to be zipped\nprint(f'Total Files: {len(file_paths)}')\n\n#Begin zipping process\nprint('Zipping...')\nfor file_path in tqdm(file_paths):\n    #Write the file to the zip archive, using a relative path within the archive\n    zipObj.write(file_path, file_path[len(IMAGE_DIR):])\n    \n    # emove the original file after adding to the zip archive\n    os.remove(file_path) if os.path.isfile(file_path) else None\n\n#Close the ZipFile object\nzipObj.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:55:16.682857Z","iopub.execute_input":"2023-10-25T18:55:16.683239Z","iopub.status.idle":"2023-10-25T18:55:19.068601Z","shell.execute_reply.started":"2023-10-25T18:55:16.683209Z","shell.execute_reply":"2023-10-25T18:55:19.067682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}